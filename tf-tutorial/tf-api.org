#+TITLE: TF API
#+AUTHOR: Wei Sun (孙伟)
#+EMAIL: waysun@amazon.com
#+DATE: <2020-07-21 Tue>
#+CATEGORY:
#+FILETAGS:

* TF API

** 生成 Tensor

*** constant

#+begin_src ipython
  import tensorflow as tf
  import numpy as np

  print(tf.constant(1))
  print(tf.constant(1, dtype=tf.int64))
  print(tf.constant(1.0))
  print(tf.constant(1.0, dtype=tf.float64))
  print(tf.constant(1.0, dtype=tf.double))
  print(tf.constant("hello world"))
  print(tf.constant(True))
  print(tf.constant([1,2,3]))
  print(tf.constant([1,2,3],dtype=tf.float32))
  print(tf.constant([1,2,3,4],shape=(2,2),dtype=tf.float32))
  print(tf.constant([[1,2],[3,4]]))
#+end_src

#+RESULTS:
:results:
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(1.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float64)
tf.Tensor(1.0, shape=(), dtype=float64)
tf.Tensor(b'hello world', shape=(), dtype=string)
tf.Tensor(True, shape=(), dtype=bool)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)
tf.Tensor(
[[1. 2.]
 [3. 4.]], shape=(2, 2), dtype=float32)
tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32)
:end:

*** eye

#+begin_src ipython
  print(tf.eye(3))
  print(tf.eye(2, 3))
  print(tf.eye(2, batch_shape=[1,2]))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]], shape=(3, 3), dtype=float32)
tf.Tensor(
[[1. 0. 0.]
 [0. 1. 0.]], shape=(2, 3), dtype=float32)
tf.Tensor(
[[[[1. 0.]
   [0. 1.]]

  [[1. 0.]
   [0. 1.]]]], shape=(1, 2, 2, 2), dtype=float32)
:end:

*** fill/ones

#+begin_src ipython
  print(tf.fill([2, 3], 9))
  print(tf.ones([2, 3]))
  print(tf.constant(9, shape=(2, 3)))
  print(tf.convert_to_tensor(np.full([2,3], 9)))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[9 9 9]
 [9 9 9]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[9 9 9]
 [9 9 9]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[9 9 9]
 [9 9 9]], shape=(2, 3), dtype=int64)
tf.Tensor(
[[1. 1. 1.]
 [1. 1. 1.]], shape=(2, 3), dtype=float32)
:end:

*** gather

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  print(tf.gather(a, [0, 1]))

  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(tf.gather(a, [0, 1], axis=1))
  print(tf.gather(a, [1, 0], axis=0))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 2], shape=(2,), dtype=int32)
tf.Tensor(
[[1 2]
 [4 5]], shape=(2, 2), dtype=int32)
tf.Tensor(
[[4 5 6]
 [1 2 3]], shape=(2, 3), dtype=int32)
:end:

*** gather_nd

# TODO

*** identity/ones_like/zeros_like

#+begin_src ipython
  # 相当于 obj.clone
  print(tf.identity([1, 2, 3]))
  print(tf.ones_like([1, 2, 3]))
  print(tf.zeros_like([1, 2, 3]))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([1 1 1], shape=(3,), dtype=int32)
tf.Tensor([0 0 0], shape=(3,), dtype=int32)
:end:

*** linspace/range

#+begin_src ipython
  print(tf.linspace(0., 9., 10))
  print(tf.range(0, 10, 3))
#+end_src

#+RESULTS:
:results:
tf.Tensor([0. 1. 2. 3. 4. 5. 6. 7. 8. 9.], shape=(10,), dtype=float32)
tf.Tensor([0 3 6 9], shape=(4,), dtype=int32)
:end:

*** one_hot

#+begin_src ipython
  a = tf.constant([1, 3, 2])
  print(tf.one_hot(a, depth=4))
  print(tf.one_hot(a, depth=4, on_value=10, off_value=1))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[0. 1. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 1. 0.]], shape=(3, 4), dtype=float32)
tf.Tensor(
[[ 1 10  1  1]
 [ 1  1  1 10]
 [ 1  1 10  1]], shape=(3, 4), dtype=int32)
:end:

*** repeat

#+begin_src ipython
  print(tf.repeat(1, 3))
  print(tf.repeat([1, 2, 3], [2, 2, 1]))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 1 1], shape=(3,), dtype=int32)
tf.Tensor([1 1 2 2 3], shape=(5,), dtype=int32)
:end:

** 类型转换

*** cast

#+begin_src ipython
  a = tf.constant([0, 2, 3])
  print(a)
  print(tf.cast(a,tf.float32))
  print(tf.cast(a,tf.bool))

  a=tf.constant([0., 1., 2.])
  print(tf.cast(a, tf.int32))
#+end_src

#+RESULTS:
:results:
tf.Tensor([0 2 3], shape=(3,), dtype=int32)
tf.Tensor([0. 2. 3.], shape=(3,), dtype=float32)
tf.Tensor([False  True  True], shape=(3,), dtype=bool)
tf.Tensor([0 1 2], shape=(3,), dtype=int32)
:end:

*** bitcast

#+begin_src ipython
  a = tf.constant([0, 1, 2], dtype=tf.int32)
  print(a)
  # 每个 int32 被转换为 [byte_0, byte_1, byte_2, byte_3]
  print(tf.bitcast(a, tf.int8))
  # int8 转换为 int32, 则 shape[-1] 需要等于 4, 因为 4 个 int8 才能当作一个 int32
  a = tf.constant([0, 1, 0, 0], dtype=tf.int8)
  print(tf.bitcast(a, tf.int32))
#+end_src

#+RESULTS:
:results:
tf.Tensor([0 1 2], shape=(3,), dtype=int32)
tf.Tensor(
[[0 0 0 0]
 [1 0 0 0]
 [2 0 0 0]], shape=(3, 4), dtype=int8)
tf.Tensor(256, shape=(), dtype=int32)
:end:

*** convert_to_tensor
#+begin_src ipython
  print(tf.convert_to_tensor(1))
  print(tf.convert_to_tensor([1,2,3]))
  print(tf.convert_to_tensor(np.array([1,2,3])))
  print(tf.convert_to_tensor(tf.constant([1, 2, 3]).numpy()))
#+end_src

#+RESULTS:
:results:
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int64)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
:end:

*** is_tensor

#+begin_src ipython
  print(tf.is_tensor(1))
  print(tf.is_tensor("hello"))
  print(tf.is_tensor([1, 2, 3]))
  print(tf.is_tensor(np.array([1, 2, 3])))
  print(tf.is_tensor(tf.convert_to_tensor([1, 2, 3])))
  print(tf.is_tensor(tf.constant(1)))
#+end_src

#+RESULTS:
:results:
False
False
False
False
True
True
:end:

** 操作大小

*** broadcast_to

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  print(tf.broadcast_to(a, (2, 3)))

  # (x0,x1,x2) 能 broadcast_to (y0,y1,y2) 的条件是 x[i] == 1 或 x[i] == y[i]
  # 所以下面的函数会失败, (2,3) 无法 broadcast_to (4,3)
  # a = tf.constant([[1, 2, 3], [4, 5, 6]])
  # print(tf.broadcast_to(a, (4, 3)))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1 2 3]
 [1 2 3]], shape=(2, 3), dtype=int32)
:end:

*** concat

#+begin_src ipython
  a = tf.constant([[1, 2, 3]])
  b = tf.constant([[4, 5, 6]])
  # concat 没有默认的 axis
  print(tf.concat([a, b],axis=0))
  print(tf.concat([a, b],axis=1))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)
tf.Tensor([[1 2 3 4 5 6]], shape=(1, 6), dtype=int32)
:end:

*** ensure_shape

#+begin_src ipython
  print(tf.ensure_shape(tf.constant([1, 2, 3, 4]), (4,)))
  print(tf.ensure_shape(tf.constant([[1, 2, 3, 4],[1, 2, 3, 4]]), (None, 4)))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)
tf.Tensor(
[[1 2 3 4]
 [1 2 3 4]], shape=(2, 4), dtype=int32)
:end:

*** expand_dims

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  print(a.shape)
  b = tf.expand_dims(a, 0)
  print(b.shape)
  b = tf.expand_dims(b, -1)
  print(b.shape)
#+end_src

#+RESULTS:
:results:
(3,)
(1, 3)
(1, 3, 1)
:end:

*** squeeze

#+begin_src ipython
  a = tf.constant([[1, 2, 3]])
  print(tf.squeeze(a).shape)

  # squeeze 默认会删掉所有 size=1 的 dimension
  a = tf.expand_dims(tf.expand_dims([1, 2, 3],0), 2)
  print(a.shape)
  print(tf.squeeze(a).shape)

  # 通过一个 list 参数可以指定去掉哪个 size=1 的 dimension
  a = tf.expand_dims(tf.expand_dims([1, 2, 3],0), 2)
  print(a.shape)
  print(tf.squeeze(a, [0]).shape)
#+end_src

#+RESULTS:
:results:
(3,)
(1, 3, 1)
(3,)
(1, 3, 1)
(3, 1)
:end:

*** reshape

*** shape/size/rank

#+begin_src ipython
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(a.shape)
  print(tf.shape(a))
  print(tf.size(a))
  print(tf.rank(a))
#+end_src

#+RESULTS:
:results:
(2, 3)
tf.Tensor([2 3], shape=(2,), dtype=int32)
tf.Tensor(6, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
:end:

*** slice

#+begin_src ipython
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  # tf.sliace(a,[f0,f1],[s0,s1]) == a[f0:f0+s0, f1:f1+s1]
  print(tf.slice(a, [0, 1], [1, 2]))
  print(a[0:1, 1:3])
#+end_src

#+RESULTS:
:results:
tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)
tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)
:end:

*** split

#+begin_src ipython
  a = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]])
  print(tf.split(a, 2, axis=1))

  # a 的 shape 不能被平均的 split 时会失败
  # a = tf.constant([1, 2, 3])
  # print(tf.split(a, 2))
#+end_src

#+RESULTS:
:results:
[<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1, 2],
       [5, 6]], dtype=int32)>, <tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[3, 4],
       [7, 8]], dtype=int32)>]
:end:

*** stack/unstack

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  b = tf.constant([4, 5, 6])
  print(tf.stack([a,b]))

  # stack 与 concat 有显著的区别:
  # stack 会创造新的一维, 而 concat 会在某一维上合并数据
  a = tf.constant([[1, 2, 3]])
  b = tf.constant([[4, 5, 6]])
  print(tf.stack([a,b], axis=0))
  print(tf.stack([a,b], axis=1))
  print(tf.concat([a, b], axis=0))
  print(tf.concat([a, b], axis=1))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[[1 2 3]]

 [[4 5 6]]], shape=(2, 1, 3), dtype=int32)
tf.Tensor(
[[[1 2 3]
  [4 5 6]]], shape=(1, 2, 3), dtype=int32)
tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)
tf.Tensor([[1 2 3 4 5 6]], shape=(1, 6), dtype=int32)
:end:

*** tile

#+begin_src ipython
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(tf.tile(a, [2, 2]))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1 2 3 1 2 3]
 [4 5 6 4 5 6]
 [1 2 3 1 2 3]
 [4 5 6 4 5 6]], shape=(4, 6), dtype=int32)
:end:

** 运算

*** equal/greater/less/greater_equal/less_equal

#+begin_src ipython
  print(tf.equal(
      tf.constant([[1, 2, 3], [1, 2, 3]]),
      tf.constant([[1, 2, 3], [1, 2, 3]])))
  # broadcast
  print(tf.equal(
      tf.constant([[1, 2, 3], [1, 2, 3]]),
      tf.constant([[1, 2, 3]])))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[ True  True  True]
 [ True  True  True]], shape=(2, 3), dtype=bool)
tf.Tensor(
[[ True  True  True]
 [ True  True  True]], shape=(2, 3), dtype=bool)
:end:

*** abs/negative

#+begin_src ipython
  print(tf.abs(tf.constant([-1,0,1])))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 0 1], shape=(3,), dtype=int32)
:end:

*** add/substract/multiply/divide/pow

#+begin_src ipython
  print("=== add")
  a = tf.constant([1, 2])
  print(tf.add(a, a))
  print(a + a)
  print(a + 1)
  print(tf.add_n([a, a, a]))

  # broadcast
  print(tf.add(tf.constant([1, 2, 3]), tf.constant([[1, 1, 1], [1, 1, 1]])))

  print("=== subtract")
  a = tf.constant([1,2,3])
  b = tf.constant([0, 1, 2])
  print(tf.subtract(a, b))
  print(a - b)

  print("=== divide")
  a = tf.constant([1, 2, 3])
  b = tf.constant([1, 2, 3])
  print(tf.divide(a, b))
  print(a / b)
  print(a / 2)
  # broadcast
  print(tf.constant([[1, 2, 3], [4, 5, 6]]) / tf.constant([1, 2, 3]))
#+end_src

#+RESULTS:
:results:
=== add
tf.Tensor([2 4], shape=(2,), dtype=int32)
tf.Tensor([2 4], shape=(2,), dtype=int32)
tf.Tensor([2 3], shape=(2,), dtype=int32)
tf.Tensor([3 6], shape=(2,), dtype=int32)
tf.Tensor(
[[2 3 4]
 [2 3 4]], shape=(2, 3), dtype=int32)
=== subtract
tf.Tensor([1 1 1], shape=(3,), dtype=int32)
tf.Tensor([1 1 1], shape=(3,), dtype=int32)
=== divide
tf.Tensor([1. 1. 1.], shape=(3,), dtype=float64)
tf.Tensor([1. 1. 1.], shape=(3,), dtype=float64)
tf.Tensor([0.5 1.  1.5], shape=(3,), dtype=float64)
tf.Tensor(
[[1.  1.  1. ]
 [4.  2.5 2. ]], shape=(2, 3), dtype=float64)
:end:

*** argmin/argmax

#+begin_src ipython
  print(tf.argmax([1, 2, 3]))
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(a.shape)
  print(tf.argmax(a))
  print(tf.argmax(a,axis=0))
  print(tf.argmax(a,axis=1))
#+end_src

#+RESULTS:
:results:
tf.Tensor(2, shape=(), dtype=int64)
(2, 3)
tf.Tensor([1 1 1], shape=(3,), dtype=int64)
tf.Tensor([1 1 1], shape=(3,), dtype=int64)
tf.Tensor([2 2], shape=(2,), dtype=int64)
:end:

*** clip_by_value

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  print(tf.clip_by_value(a, 1, 2))
  # min, max 可以是一个 shape 为 a.shape 的 tensor
  print(tf.clip_by_value(a, [1, 2, 3], 3))
  #或者可以 broadcast_to (a) 
  print(tf.clip_by_value(a, [1], 3))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 2 2], shape=(3,), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
:end:

*** argsort/sort

#+begin_src ipython
  a = tf.constant([3, 2, 1])
  print(tf.argsort (a))
  print(tf.argsort(a, direction="DESCENDING"))

  a = tf.constant([[3, 2, 1], [1, 2, 3]])
  print(tf.argsort (a))
  # argsort 默认的 axis 是 -1 而不是 0
  print(tf.argsort (a, axis=-1))
  print(tf.argsort (a, axis=0))


  a = tf.constant([[3, 2, 1], [1, 2, 3]])
  print(tf.sort(a))
#+end_src

#+RESULTS:
:results:
tf.Tensor([2 1 0], shape=(3,), dtype=int32)
tf.Tensor([0 1 2], shape=(3,), dtype=int32)
tf.Tensor(
[[2 1 0]
 [0 1 2]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[2 1 0]
 [0 1 2]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[1 0 0]
 [0 1 1]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[1 2 3]
 [1 2 3]], shape=(2, 3), dtype=int32)
:end:

*** cumsum

#+begin_src ipython
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(tf.cumsum(a))
  print(tf.cumsum(a, axis=1))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1 2 3]
 [5 7 9]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[ 1  3  6]
 [ 4  9 15]], shape=(2, 3), dtype=int32)
:end:

*** floor/round

#+begin_src ipython
  print(tf.floor([1.1, 2.1, 3.1]))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)
:end:

*** dynamic_partition

#+begin_src ipython
  a = tf.constant([1, 2, 3, 4])
  p = tf.constant([0, 1, 0, 1])
  print(tf.dynamic_partition(a, p, 2))

  # dynamic_partition 返回 n 个向量, 不论 a 在 shape 是什么
  a = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]])
  p = tf.constant([[0, 1, 0, 1], [0, 1, 0, 1]])
  print(tf.dynamic_partition(a, p, 2))
#+end_src

#+RESULTS:
:results:
[<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 4], dtype=int32)>]
[<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 3, 5, 7], dtype=int32)>, <tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 4, 6, 8], dtype=int32)>]
:end:

*** dynamic_stitch

# TODO

*** logical_and/logical_or/logical_not

*** matmul

#+begin_src ipython
  a = tf.constant([[1, 2, 3]])
  b = tf.constant([[1, 2, 3]])
  print(tf.matmul(a, b, transpose_b=True))

  # broadcast
  a = tf.constant([[1, 2, 3], [1, 2, 3]])
  b = tf.constant([[1], [2], [3]])
  print(tf.matmul(a, b))

  print(a @ b)
#+end_src

#+RESULTS:
:results:
tf.Tensor([[14]], shape=(1, 1), dtype=int32)
tf.Tensor(
[[14]
 [14]], shape=(2, 1), dtype=int32)
tf.Tensor(
[[14]
 [14]], shape=(2, 1), dtype=int32)
:end:


*** maximum/minimum

*** norm

*** reduce_{all,any,max,min,mean,prod,sum}

#+begin_src ipython
  print("=== reduce_all")
  print(tf.reduce_all(tf.constant([True, True, False])))
  a = tf.constant([[True, True], [False, False]])
  print(tf.reduce_all(a))
  print(tf.reduce_all(a, axis=0))
  print(tf.reduce_all(a, axis=1))
  print("=== reduce_sum")
  print(tf.reduce_sum(tf.constant([1, 2, 3])))
#+end_src

#+RESULTS:
:results:
=== reduce_all
tf.Tensor(False, shape=(), dtype=bool)
tf.Tensor(False, shape=(), dtype=bool)
tf.Tensor([False False], shape=(2,), dtype=bool)
tf.Tensor([ True False], shape=(2,), dtype=bool)
=== reduce_sum
tf.Tensor(6, shape=(), dtype=int32)
:end:

*** reverse

#+begin_src ipython
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(tf.reverse(a, axis=[0]))
  print(tf.reverse(a, axis=[0, 1]))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[4 5 6]
 [1 2 3]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[6 5 4]
 [3 2 1]], shape=(2, 3), dtype=int32)
:end:

*** roll

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  print(tf.roll(a, shift=2, axis=0))

  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(tf.roll(a, shift=[1, 2], axis=[0, 1]))
#+end_src

#+RESULTS:
:results:
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor(
[[5 6 4]
 [2 3 1]], shape=(2, 3), dtype=int32)
:end:

*** sigmoid

*** transpose

#+begin_src ipython
  a = tf.constant([[1, 2, 3], [4, 5, 6]])
  print(tf.transpose(a))
  print(tf.transpose(a, [1, 0]))
  print(tf.transpose(a, [0, 1]))

  a = tf.constant([[[ 1,  2,  3],
                    [ 4,  5,  6]],
                   [[ 7,  8,  9],
                    [10, 11, 12]]])

  print(tf.transpose(a, [0, 2, 1]))
#+end_src

#+RESULTS:
:results:
tf.Tensor(
[[1 4]
 [2 5]
 [3 6]], shape=(3, 2), dtype=int32)
tf.Tensor(
[[1 4]
 [2 5]
 [3 6]], shape=(3, 2), dtype=int32)
tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)
tf.Tensor(
[[[ 1  4]
  [ 2  5]
  [ 3  6]]

 [[ 7 10]
  [ 8 11]
  [ 9 12]]], shape=(2, 3, 2), dtype=int32)
:end:

*** truncatemod/truncatediv

#+begin_src ipython
  a = tf.constant([1, 2, 3])
  print(tf.truncatemod(a, 3))
#+end_src

#+RESULTS:
:results:
tf.Tensor([1 2 0], shape=(3,), dtype=int32)
:end:

*** unique/unique_with_counts

#+begin_src ipython
  print(tf.unique([1, 2, 3, 1]))
  # unique 只能用于 1-d tensor
  # print(tf.unique([[1, 2, 3, 1]]))
#+end_src

#+RESULTS:
:results:
Unique(y=<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>, idx=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 0], dtype=int32)>)
:end:

